{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a75522c0-8bcd-4b1a-90a3-f37132333531",
   "metadata": {},
   "source": [
    "## Step by step Implementation of Vision Transformers in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5cd935-301b-4345-81a9-ec1de4771dcf",
   "metadata": {},
   "source": [
    "In this notebook, I systematically implemented the stages of the Vision Transformers (ViT) model, combining them to construct the entire ViT architecture. The figure above, sourced from the original ViT paper titled \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\" (ICLR 2021), illustrates crucial components like image patches, positional embeddings, learnable class embeddings, linear projection, multi-head attention, and MLP head.\n",
    "<div style=\"text-align:center\">\n",
    "    <img src='./images/vit_overall.png' width=90% height=90%/>\n",
    "</div>\n",
    "After reviewing this notebook, you will gain a clear understanding of each stage and be able to answer the following questions:\n",
    "\n",
    "- Overview of the ViT architecture\n",
    "- Explanation of image patches, class tokens, positional embeddings, Self-Attention, Multi-Head Attention, Linear Projection\n",
    "- Process of creating image patches\n",
    "- Initialization of class tokens and positional embeddings and their combination with image patches\n",
    "- Mathematical details of the Self-Attention mechanism\n",
    "- Construction of the Multi-Head Attention model from the Self-Attention layer/function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7d222a-d71d-4d05-9a5d-b34e5aeb8711",
   "metadata": {},
   "source": [
    "The entire implementation is carried out using the Torch framework, and the Pillow library is utilized for loading and resizing images to the desired dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "190a4e9f-4fb2-40cb-9a8e-afb971af2dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import math\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms as T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea355405-3160-4379-8787-68ae8ef8437d",
   "metadata": {},
   "source": [
    "The authors conducted experiments using three different architectures by modifying the parameters, as indicated in the table below. In this notebook, ViT-Base will be implemented, and the parameters initialized in the next cell are configured accordingly for that model.\n",
    "<div style=\"text-align:center\">\n",
    "    <img src='./images/vit_variants.png' width=50% height=50%/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29500966-6015-42c1-8b80-9df28d75a43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce some parameters\n",
    "desired_image_size = 224   # The desired size is 224 as mentioned in the paper\n",
    "B = batch_size = 2                        # Batch Size\n",
    "P = patch_size = 16                       # patch size\n",
    "C = num_channels = 3                      # number of channels (RGB)\n",
    "D = embedding_dim = 768                   # dimension of the patch embeddings\n",
    "num_layers = 12                           # Number of layers in ViT model\n",
    "num_heads = 12                            # Number of heads in ViT model\n",
    "hidden_dim = 3072                         # hidden layer dimension in MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad3b597b-c801-40cb-9eeb-f4cd80f0a359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size:(224, 224) and mode:RGB. Image should be an RGB image with size of (224,224).\n"
     ]
    }
   ],
   "source": [
    "# Load an image from a file and resize it.\n",
    "image = Image.open('./images/photo.jpg')\n",
    "image = image.resize((desired_image_size, desired_image_size))\n",
    "print(f'Image size:{image.size} and mode:{image.mode}. Image should be an RGB image with size of (224,224).')\n",
    "assert image.size == (224, 224)\n",
    "assert image.mode == 'RGB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10a20068-9dfc-4330-a1a0-5d53a9fbcc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the tensor, (B, C, W, H): [2, 3, 224, 224]\n"
     ]
    }
   ],
   "source": [
    "# Transform the Pillow image into a Torch tensor and generate a batch by replicating it. Initiating with a batched tensor is crucial \n",
    "# to ensure that our implementation accommodates batches.\n",
    "\n",
    "image_tensor = T.PILToTensor()(image) # torch tensor\n",
    "\n",
    "image_batch = torch.stack([image_tensor for _ in range(B)])\n",
    "print(f'Shape of the tensor, (B, C, W, H): {list(image_batch.shape)}')\n",
    "assert list(image_batch.shape) == [2, 3, 224, 224]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3306e3f1-a365-44d1-a014-d4f335ca588f",
   "metadata": {},
   "source": [
    "Figure below shows the mathematical calculation of the image patches only for one image. In the implementation, batch supported version is implemented.\n",
    "<div style=\"text-align:center\">\n",
    "    <img src='./images/image_patches.png' width=90% height=90%/>\n",
    "</div>\n",
    "Patches are generated from the given image with a size of 16x16 without overlapping. In this case, the number of patches created is calculated as (224/16) * (224/16) = 196. As the transformer architecture doesn't accept 3D/4D inputs, each patch is flattened and stacked together. This results in 2D patches with a size of (196, (patch_size * patch_size * number of channels)). \n",
    "\n",
    "For our configuration, this translates to (196, (16*16*3)) = (196, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "775935ab-d2a3-437e-bb59-1ac7676f49dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of patches and patch_dimension to use later as the expected values\n",
    "number_of_patches = int((desired_image_size * desired_image_size)/ (P * P))\n",
    "assert number_of_patches == 196\n",
    "patch_dim = int(P * P * C)\n",
    "assert patch_dim == 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9b34f4c-767a-47bd-afaa-eb23a35cc401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of one flattened Patch: torch.Size([196, 768])\n"
     ]
    }
   ],
   "source": [
    "# Create patches and apply flatten operation\n",
    "# With the batch support the operation becomes this in Torch\n",
    "# Credit: # https://discuss.pytorch.org/t/how-to-extract-patches-from-an-image/79923/4\n",
    "unfolded_batch = image_batch.unfold(1, C, C).unfold(2, P, P).unfold(3, P, P) \n",
    "flattened_patches = unfolded_batch.contiguous().view(B, -1, C * P * P).float()\n",
    "\n",
    "print(f\"Shape of one flattened Patch: {flattened_patches[0].shape}\")\n",
    "assert list(flattened_patches.shape) == [B, number_of_patches, patch_dim]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e52fbae-5c19-41bb-a460-4f0fe6ac37a8",
   "metadata": {},
   "source": [
    "After obtaining flattened patches, patch embeddings are generated by multiplying the patches with learnable parameters. This multiplication transforms the dimension of the patches to the desired dimension, denoted as D. Following this, similar to the approach in the BERT paper, learnable class tokens are prepended to the patch embeddings. Lastly, learnable positional embeddings are added, and the output of this process serves as the input to the Transformer Encoder model.\n",
    "<div style=\"text-align:center\">\n",
    "    <img src='./images/transformer_input.png' width=100% height=100% />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59922f2-8d26-429b-b56c-30cb1deb65ad",
   "metadata": {},
   "source": [
    "The Linear Projection step involves transforming flattened patch embeddings into a higher-dimensional space using a learnable weight matrix and bias vector. This process enhances the model's capacity to capture complex patterns and features in the image. The output of this step, along with learnable class tokens and positional embeddings, serves as the input to the Transformer Encoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94f92533-a168-400f-97d9-17c0156c2581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Embeddings Shape after multiplying flattened patches with learnable parameter: [2, 196, 768]\n"
     ]
    }
   ],
   "source": [
    "# Linear Projection process\n",
    "patch_weights = nn.Parameter(torch.empty(P * P * C, D).normal_(std=0.02))\n",
    "\n",
    "patch_embeddings = torch.matmul(flattened_patches, patch_weights)\n",
    "print(f'Patch Embeddings Shape after multiplying flattened patches with learnable parameter: {list(patch_embeddings.shape)}')\n",
    "assert patch_embeddings.shape == (B, number_of_patches, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea4d491-dcfb-46e1-885a-62428172a732",
   "metadata": {},
   "source": [
    "The class token serves as a learnable parameter that provides a global representation for the entire image. It captures global information and is combined with patch embeddings before being input to the Transformer Encoder. The class token aids in developing a semantic understanding of the image, complementing local features and providing a holistic context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9f3523e-1e29-4d7c-b79f-471a1a66addf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Embeddings + Class Token should have a shape of (B, number_of_patches + 1, D): [2, 197, 768]\n"
     ]
    }
   ],
   "source": [
    "# Initialisation of Class Token\n",
    "class_token = nn.Parameter(torch.empty(1, 1, D).normal_(std=0.02))\n",
    "\n",
    "batch_class_token = class_token.expand(B, -1, -1)\n",
    "patch_embeddings_with_class_token = torch.cat([batch_class_token, patch_embeddings], dim=1)\n",
    "\n",
    "print(f'Patch Embeddings + Class Token should have a shape of (B, number_of_patches + 1, D): {list(patch_embeddings_with_class_token.shape)}')\n",
    "assert patch_embeddings_with_class_token.shape == (B, number_of_patches + 1, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e322f0a4-539f-49c7-826e-044fda6c766a",
   "metadata": {},
   "source": [
    "Position embeddings are learnable vectors that encode the spatial positions of patches in the input sequence. Added to patch embeddings, they provide crucial spatial information, allowing the model to understand the sequential order and relationships between patches. Position embeddings enable the Transformer to attend to spatial positions during self-attention, enhancing the model's ability to capture contextual information in visual tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0214587c-50a0-4dc8-9df2-62009f153a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape which are ready to be fed to Transformer model: torch.Size([2, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "# Addition of the Positional Embeddings to Patch Embeddings with Class Tokens.\n",
    "positional_embedding = nn.Parameter(torch.empty(B, number_of_patches + 1, D).normal_(std=0.02))\n",
    "\n",
    "embeddings = patch_embeddings_with_class_token + positional_embedding\n",
    "\n",
    "print(f'Embeddings shape which are ready to be fed to Transformer model: {embeddings.shape}')\n",
    "assert embeddings.shape == (B, number_of_patches + 1, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4135be34-6563-4f87-a341-166dfad87c56",
   "metadata": {},
   "source": [
    "Self-Attention mechanism allows the model to weigh the importance of different elements in a sequence when processing each element.\n",
    "It contains three vectors called Key, Query, and Value Vectors. \n",
    "Each input element (token or patch) is associated with three vectors:\n",
    "- Query (Q): Represents the element's information.\n",
    "- Key (K): Helps the model understand the relationships between elements.\n",
    "- Value (V): Holds the actual information associated with the element.\n",
    "<div style=\"text-align:center\">\n",
    "    <img src='./images/selfattention.png' width=100% height=100% />\n",
    "</div>\n",
    "Scores are computed by dot products of Q and K, then scaled and softmaxed to obtain attention weights. These weights determine the importance of each element. The weighted sum of Values produces a context vector.\n",
    "\n",
    "Self-Attention enables the model to understand spatial dependencies and features in image patches, considering both local and global context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c163f995-f19b-4bd1-aa8b-11c97494a3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Attention values: torch.Size([2, 197, 64])\n"
     ]
    }
   ],
   "source": [
    "# Functional implementation which can't be attached to the Torch Module. \n",
    "qkv_dim = int(D / num_heads) \n",
    "# The dimensions of the Query, Key, and Value matrices are determined by the embedding dimension. \n",
    "# Ignoring the batch dimension, their first dimensions match the embedding dimension, while \n",
    "# the second dimension is calculated based on the number of heads in the architecture.\n",
    "\n",
    "W = nn.Parameter(torch.empty(1, D, int(3 * qkv_dim)).normal_(std=0.02))\n",
    "\n",
    "# calculate query, key and value projection\n",
    "qkv = torch.matmul(embeddings, W)\n",
    "q = qkv[:, :, :qkv_dim]\n",
    "k = qkv[:, :, qkv_dim:qkv_dim*2 ]\n",
    "v = qkv[:, :, qkv_dim*2:]\n",
    "\n",
    "# Calculate attention weights by applying a softmax to the dot product of all queries with all keys\n",
    "attention_weights = F.softmax(torch.matmul(q, torch.transpose(k, -2, -1) ) / math.sqrt(qkv_dim), dim=1)\n",
    "\n",
    "# calculate attention values\n",
    "attention_values = torch.matmul(attention_weights, v)\n",
    "\n",
    "print(f'Shape of the Attention values: {attention_values.shape}')\n",
    "assert attention_values.shape == (B, number_of_patches + 1, qkv_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe786c1-0bf0-4cef-b7f9-4bad9227912b",
   "metadata": {},
   "source": [
    "As Self-Attention is used several times in Multi-Head Attention, it will be wise to convert it to a Torch Module as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8e81020-e3c8-4879-b606-2cd473f73b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self Attention Module\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, qkv_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim   # embedding dimension\n",
    "        self.qkv_dim = qkv_dim               # Dimension of key, query, value\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(1, embedding_dim, int(3 * qkv_dim)).normal_(std=0.02))\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "\n",
    "        # calculate query, key and value projection\n",
    "        qkv = torch.matmul(embeddings, self.W)\n",
    "        q = qkv[:, :, :self.qkv_dim]\n",
    "        k = qkv[:, :, self.qkv_dim:self.qkv_dim*2 ]\n",
    "        v = qkv[:, :, self.qkv_dim*2:]\n",
    "        \n",
    "        # Calculate attention weights by applying a softmax to the dot product of all queries with all keys\n",
    "        attention_weights = F.softmax(torch.matmul(q, torch.transpose(k, -2, -1) ) / math.sqrt(self.qkv_dim), dim=1)\n",
    "        \n",
    "        # calculate attention values and return\n",
    "        return torch.matmul(attention_weights, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "077d74fd-072f-4b92-964e-79056be9d7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Attention values: torch.Size([2, 197, 192])\n"
     ]
    }
   ],
   "source": [
    "# initialise self-attention object\n",
    "self_attention = SelfAttention(embedding_dim=D, qkv_dim=int(3 * qkv_dim)) \n",
    "\n",
    "attention_values = self_attention(embeddings)\n",
    "\n",
    "print(f'Shape of the Attention values: {attention_values.shape}')\n",
    "assert attention_values.shape == (B, number_of_patches + 1, int(3 * qkv_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce8e323-be5d-4589-83f5-cc8050278da7",
   "metadata": {},
   "source": [
    "Multi-head attention module enhances the model's capability by allowing it to focus on various aspects of input patches simultaneously. It involves linear projections of Query (Q), Key (K), and Value (V) vectors for each attention head (as implemented above), followed by independent scaled dot-product attention computations. The outputs from different heads are concatenated, linearly transformed, and serve as the final result, enabling the model to capture diverse features and relationships in the input image patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49b1791b-ce17-4c3b-8046-dfc2fbf98878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head Attention Module\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.num_heads = num_heads            \n",
    "        self.embedding_dim = embedding_dim    # embedding dimension\n",
    "\n",
    "        self.qkv_dim = embedding_dim // num_heads   # Dimension of key, query, and value can be calculated with embedding_dim and num_of_heads\n",
    "\n",
    "        # initialise self-attention modules num_heads times\n",
    "        self.multi_head_attention = nn.ModuleList([SelfAttention(embedding_dim, self.qkv_dim) for _ in range(num_heads)])\n",
    "\n",
    "        # initialise weight matrix. \n",
    "        self.W = nn.Parameter(torch.empty(1, num_heads * self.qkv_dim, embedding_dim).normal_(std=0.02))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # self-attention scores for each head\n",
    "        attention_scores = [attention(x) for attention in self.multi_head_attention]\n",
    "\n",
    "        # The outputs from all attention heads are concatenated and linearly transformed. \n",
    "        Z = torch.cat(attention_scores, -1)\n",
    "\n",
    "        # This step ensures that the model can consider a comprehensive set of relationships captured by different heads.\n",
    "        return torch.matmul(Z, self.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a73a3177-257d-4c19-9dc6-fe7bc3bdd2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Multi-Head Attention: torch.Size([2, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "# initialise Multi-Head Attention object\n",
    "multi_head_attention = MultiHeadAttention(D, num_heads)\n",
    "\n",
    "# calculate Multi-Head Attention score\n",
    "multi_head_attention_score = multi_head_attention(patch_embeddings_with_class_token)\n",
    "\n",
    "print(f'Shape of the Multi-Head Attention: {multi_head_attention_score.shape}')\n",
    "assert multi_head_attention_score.shape == (B, number_of_patches + 1, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558b606f-ce6c-44e3-9628-1fb4445d0077",
   "metadata": {},
   "source": [
    "The MLP module enhances the model's representation by applying linear transformations, non-linear activation functions, and a final linear transformation to the multi-head attention scores. \n",
    "It introduces non-linearity, enabling the model to capture complex patterns and relationships within the data.\n",
    "This step further refines the representation and prepares it for subsequent layers in the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d803538b-c659-492e-ae83-1f916699bf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "                            nn.Linear(embedding_dim, hidden_dim),\n",
    "                            nn.GELU(),\n",
    "                            nn.Linear(hidden_dim, embedding_dim)\n",
    "                   )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c240068f-8a9e-442e-8200-91b8d5ace741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of MLP output: torch.Size([2, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "# initialise MLP object\n",
    "mlp = MLP(D, hidden_dim)\n",
    "\n",
    "output = mlp(multi_head_attention_score)\n",
    "\n",
    "assert output.shape == (B, number_of_patches + 1, D)\n",
    "print(F'Shape of MLP output: {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9c63c3-6b08-43c2-9b52-996ca1f7c039",
   "metadata": {},
   "source": [
    "Having implemented all the essential functionalities step by step, we can now introduce the Transformer Encoder Module by combining the necessary functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9021d957-1f39-43b6-a964-a874388aa168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Encoder Module\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, hidden_dim, dropout):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        self.multi_head_attention = MultiHeadAttention(embedding_dim, num_heads)\n",
    "        self.mlp = MLP(embedding_dim, hidden_dim)\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(p=dropout)\n",
    "        self.dropout2 = nn.Dropout(p=dropout)\n",
    "        self.dropout3 = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        # Applying dropout\n",
    "        dropout_embeddings = self.dropout1(embeddings)\n",
    "        # Layer normalization\n",
    "        normalized_embeddings = self.layer_norm1(dropout_embeddings)\n",
    "        # Calculation of multi-head attention\n",
    "        attention_scores = self.multi_head_attention(normalized_embeddings)\n",
    "        # Applying the second dropout\n",
    "        dropout_attention_scores = self.dropout2(attention_scores)\n",
    "        # Residual connection with second dropout output and initial input\n",
    "        residuals_embeddings = embeddings + dropout_attention_scores\n",
    "        # apply layer normalization\n",
    "        normalized_residuals_embeddings = self.layer_norm2(residuals_embeddings)\n",
    "        # aply MLP \n",
    "        transformed_results = self.mlp(normalized_residuals_embeddings)\n",
    "        # Applying the third dropout\n",
    "        dropout_transformed_results = self.dropout3(transformed_results)\n",
    "        # Residual connection with last dropout output and first residual output\n",
    "        output = residuals_embeddings + dropout_transformed_results\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9cebbade-eb5e-45b2-aa9a-a87723cdfa94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the output of Transformer Encoders: torch.Size([2, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "# init transformer encoder\n",
    "transformer_encoder = TransformerEncoder(embedding_dim=D, num_heads=num_heads, hidden_dim=hidden_dim, dropout=0.1)\n",
    "\n",
    "# compute transformer encoder output\n",
    "output = transformer_encoder(embeddings)\n",
    "\n",
    "print(f'Shape of the output of Transformer Encoders: {output.shape}')\n",
    "assert output.shape == (B, number_of_patches + 1, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fd9642-4f6e-4d14-9056-102048287027",
   "metadata": {},
   "source": [
    "The output of the Transformer Encoder is a sequence of feature vectors, each corresponding to a specific patch of the input image. These feature vectors capture hierarchical and contextual information learned during the self-attention mechanism within the transformer layers. The final feature sequence is then used for downstream tasks such as classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8783d5b-f195-46e3-ad96-482f5d8a4c21",
   "metadata": {},
   "source": [
    "The MLP Head processes the transformer encoder's output by applying global average pooling, a fully connected layer, an activation function (typically softmax), and produces the final probability distribution over classes for classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4358576f-573f-4de2-abfa-36a0a646c062",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_classes, is_train=True):\n",
    "        super(MLPHead, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        # this part is taken from torchvision implementation\n",
    "        if is_train:\n",
    "            self.head = nn.Sequential(\n",
    "                                    nn.Linear(embedding_dim, 3072),  # hidden layer\n",
    "                                    nn.Tanh(),\n",
    "                                    nn.Linear(3072, num_classes)    # output layer\n",
    "                            )\n",
    "        else:\n",
    "            # single linear layer\n",
    "            self.head = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "481d4b4a-a5e5-409d-b5f9-c74939989576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the MLP Head output: torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "# Classifier \"token\" as used by standard language architectures\n",
    "class_token_output = output[:, 0] \n",
    "\n",
    "# initialise number of classes\n",
    "n_class = 10\n",
    "\n",
    "# initialise classification head \n",
    "mlp_head = MLPHead(D, n_class)\n",
    "\n",
    "cls_output = mlp_head(class_token_output)\n",
    "\n",
    "# size of output\n",
    "print(f'Shape of the MLP Head output: {cls_output.shape}')\n",
    "assert list(cls_output.shape) == [B, n_class]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9682595-9f0e-4b44-b082-44c45336fb0a",
   "metadata": {},
   "source": [
    "At last, implementing the Vision Transformer module which involves utilizing all the functions discussed above in accordance with the visual representations and information provided in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "afa71f74-3019-4d35-b56f-02f672915d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VisionTranformer Module\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, patch_size=16, image_size=224, C=3,\n",
    "                     num_layers=12, embedding_dim=768, num_heads=12, hidden_dim=3072,\n",
    "                            dropout_prob=0.1, num_classes=10):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.C = C\n",
    "\n",
    "        # get the number of patches of the image\n",
    "        self.num_patches = int(image_size ** 2 / patch_size ** 2) # (width * height) / (patch_size**2)\n",
    "\n",
    "        # trainable linear projection for mapping dimension of patches (weight matrix E)\n",
    "        self.W = nn.Parameter(torch.empty(1, patch_size * patch_size * C, embedding_dim).normal_(std=0.02))\n",
    "\n",
    "        # position embeddings\n",
    "        self.positional_embeddings = nn.Parameter(torch.empty(1, self.num_patches + 1, embedding_dim).normal_(std=0.02))\n",
    "\n",
    "        # learnable class tokens\n",
    "        self.class_tokens = nn.Parameter(torch.rand(1, D))\n",
    "\n",
    "        # transformer encoder\n",
    "        self.transformer_encoder = nn.Sequential(*[\n",
    "            TransformerEncoder(embedding_dim, num_heads, hidden_dim, dropout_prob) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # mlp head\n",
    "        self.mlp_head = MLPHead(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, images):\n",
    "        # get patch size and channel size\n",
    "        P, C = self.patch_size, self.C\n",
    "\n",
    "        # create image patches\n",
    "        patches = images.unfold(1, C, C).unfold(2, P, P).unfold(3, P, P).contiguous().view(images.size(0), -1, C * P * P).float()\n",
    "\n",
    "        # patch embeddings\n",
    "        patch_embeddings = torch.matmul(patches , self.W)\n",
    "\n",
    "        # class token + patch_embeddings\n",
    "        batch_class_token = self.class_tokens.expand(patch_embeddings.shape[0], -1, -1)\n",
    "        patch_embeddings_with_class_token = torch.cat([batch_class_token, patch_embeddings], dim=1)\n",
    "\n",
    "        # add positional embedding\n",
    "        embeddings = patch_embeddings_with_class_token + self.positional_embeddings\n",
    "\n",
    "        # execute Transformer encoders\n",
    "        transformer_encoder_output = self.transformer_encoder(embeddings)\n",
    "\n",
    "        # Classifier \"token\" as used by standard language architectures\n",
    "        output_class_token = transformer_encoder_output[:, 0]\n",
    "\n",
    "        return self.mlp_head(output_class_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5fde6af-ee58-4d87-bcc2-98af6c3638ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "# init vision transformer model\n",
    "vision_transformer = VisionTransformer(patch_size=P, \n",
    "                                       image_size=desired_image_size, \n",
    "                                       C=C,\n",
    "                                       num_layers=num_layers, \n",
    "                                       embedding_dim=embedding_dim, \n",
    "                                       num_heads=num_heads, \n",
    "                                       hidden_dim=hidden_dim, \n",
    "                                       dropout_prob=0.1, \n",
    "                                       num_classes=10)\n",
    "\n",
    "# we can use image_batch as it is\n",
    "vit_output = vision_transformer(image_batch)\n",
    "\n",
    "assert vit_output.size(dim=1) == n_class\n",
    "print(vit_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2349fb-0ff5-43ea-a532-1c4fda4b7580",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
